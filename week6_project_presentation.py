# -*- coding: utf-8 -*-
"""Week6_Project_Presentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-Wtf7dNmQ55zPJAkoNonuXyKsQMi91h

# Software Coaching for Python
# Week 6: Project Presentation
"""

from google.colab import drive

drive.mount('/content/drive')
path = "/content/drive/My Drive/PythonCoaching"
my_folder = "dayoung21/project"     # *** REPLACE WITH YOUR FOLDER NAME ***
outcome_folder = f"{path}/{my_folder}/outcome"
classdata_folder = f"{path}/classdata"

"""## Executive Summary

### Name

이강표

### Target Web Site

FiveThirtyEight.com: http://fivethirtyeight.com/
(*** 발표 시에 웹사이트에 들어가서 예시 웹페이지 반드시 보여줄 것!)
"""

from IPython.display import Image
Image(f"{path}/{my_folder}/Capture.JPG")

"""### Reasons Why I Chose the Web Site

FiveThirtyEight.com은 본인이 평소 즐겨찾는 사이트로서, 정치, 스포츠, 과학 등 다양한 분야의 주제들을 "데이터"와 "통계분석"을 통해 심도 있게 다루는 기사들이 많이 올라옴. 이에 최소 1년치에 해당하는 많은 기사들을 수집해 다양한 텍스트 분석을 해보고자 하였음.

### Total Number of Web Articles Collected

1,347 건

### Data Time Range

2021-09-30 - 2023-01-10 (약 16개월)

### Key Findings and Lessons

- FiveThirtyEight.com 사이트의 특성상 예상한 바와 같이 정치와 스포츠 분야에 집중된 관심사들이 다양한 텍스트 분석에서 고르게 반영되었음.  
- 특히 2022년 가을 미 중간선거라는 중대한 기간을 앞두고 가장 많은 글들이 집중적으로 게재되었음을 확인함.

### Major Challenges

- 미국의 정치와 스포츠에 대한 상당한 도메인 지식을 갖추고 있어야만 정확한 분석과 해석이 가능했음.
- 키워드 분석에 있어 stopword로 사전 정의하기 애매한 단어들이 다수 등장하여 유의미한 키워드들을 밝혀내는 작업을 어렵게 만들었음.  
- topic modeling이 distinctive하게 나눠지지 않았음음

### Future Work

- 키워드 분석에서 어떤 단어의 단수 형태와 복수 형태가 서로 다른 단어로 구별되는 문제가 있었음. 수업 시간에 배운 word stemming 기술을 적용하여 동일한 단어로 취급해볼 여지가 있음. 
- 하나의 단어가 여러 가지 의미를 가질 수 있기 때문에, 키워드 랭킹에서 등장한 의미가 불명확한 단어들의 정확한 의미를 밝혀내면 더 정확한 분석을 할 수 있으리라 생각됨. (가령 win이나 lose가 경기 결과와 관련된 표현인지 아니면 선거 결과와 관련된 표현인지)
- Time series 분석에서 peak week 분석 뿐만 아니라 시간에 따른 키워드 랭킹의 변화를 추적해봄으로써 관심사가 어떻게 변하는지를 살펴보는 것도 의미가 있으리라 생각됨.
- 데이터의 time range를 현재 16개월에서 수 년으로 확장해보는 것도 의미 있는 시도일 것임. 
- 정치와 스포츠 분야 이외에도 데이터 및 통계 분석이 적극적으로 활용될 수 있는 영역이 무엇일지 고민.

## Installing Necessary Packages
"""

! pip install --user nltk pyldavis scikit-learn==1.0.2 textblob

"""## Loading Data"""

import pandas as pd
pd.set_option('display.max_colwidth', 150)

df = pd.read_csv(f"{outcome_folder}/example2.csv", sep="\t")
df = df.dropna(subset=["title", "body"], axis=0)
df.datetime = df.datetime.astype("datetime64")
df

"""(*** 각 column의 의미에 대해 설명한 뒤 한 row를 예제로 설명할 것!)

https://fivethirtyeight.com/features/the-red-sox-seemed-unstoppable-then-the-astros-turned-the-tables/
"""

df.info()

"""5개의 컬럼 중 author 컬럼에 소수 비어있는 값들이 존재. 이는 FiveThirtyEight.com에 게재된 글들 자체에 author 정보가 존재하지 않기 때문.

## Popular Keywords Ranking
"""

# Commented out IPython magic to ensure Python compatibility.
import nltk
nltk.download(['punkt', 'averaged_perceptron_tagger', 'stopwords'])

# %time df["words"] = df.body.apply(lambda x: nltk.word_tokenize(x))
# %time df["tagged_words"] = df.words.apply(lambda x: nltk.pos_tag(x))

df

df.to_csv(f"{outcome_folder}/example.csv", sep="\t", index=False)

df2 = pd.read_csv(f"{outcome_folder}/example.csv", sep="\t")
df2

from collections import Counter

def get_counter(dataframe, stopwords=[], target_tag=None):
    counter = Counter()
    
    for l in dataframe.tagged_words:
        word_set = set()

        for t in l:
            word = t[0].lower()
            tag = t[1]

            ##########################################################
            # Check if the word is a stopword.
            ##########################################################
            if word in stopwords:
                continue

            if target_tag is None:
                word_set.add(word)
            else:
                ##########################################################
                # Check the tag
                ##########################################################
                if tag.startswith(target_tag):
                    word_set.add(word)
                else:
                    continue

        counter.update(word_set)     # Be aware of the indentation!
        
    return counter

from nltk.corpus import stopwords
global_stopwords = nltk.corpus.stopwords.words("english") 

import string
local_stopwords = [c for c in string.punctuation] + ['‘', '’', '”', '“',  '—', '…', "could", "might", "would", "also", "among", "r"]

"""### Overall Popular Keywords"""

counter_all = get_counter(df, global_stopwords+local_stopwords)
counter_all.most_common(50)

"""전체 워드들을 대상으로 한 키워드 랭킹은 stopword로서 제외하기에 애매한 키워드들이 다수 포함된 관계로 특별히 인상 깊지는 않았음. 본문 보다는 제목을 대상으로 한 키워드 분석이 오히려 유의미한 결과를 보여줄 가능성도 있음."""

from IPython.display import Image
from wordcloud import WordCloud 

def draw_wordcloud(counter, image_file_name, max_words=100):
    wc = WordCloud(background_color="white", max_words=max_words, width=800, height=500)
    wc.generate_from_frequencies(counter)
    wc.to_file(image_file_name)
    display(Image(filename=image_file_name))
    
draw_wordcloud(counter_all, f"{outcome_folder}/week6_wordcloud_all.png", 100)

"""### Popular Nouns Representing Hot Topics"""

counter_nouns = get_counter(df, global_stopwords+local_stopwords, "NN")
counter_nouns.most_common(50)

"""명사들만을 대상으로 한 키워드 분석에서는 time(s), year(s), week, season 등과 같은 시간 관련 표현들이 상위권에 등장했고, 그밖에 team(s), game(s), season, league, player(s) 등과 같은 스포츠 용어, politics, president, trump, election, democrats, biden, republicans 등과 같은 정치 관련 표현들이 예상했던 바와 같이 자주 등장하였음.   """

draw_wordcloud(counter_nouns, f"{outcome_folder}/week6_wordcloud_nouns.png", 100)

"""### Popular Adjectives Representing Emotional Aspects"""

counter_adjs = get_counter(df, global_stopwords+local_stopwords, "JJ")
counter_adjs.most_common(50)

"""형용사들만을 대상으로 한 키워드 분석에서는 통계 전문 사이트의 특성상 last, first, many, least, much, best, big(gest), second, better, less, top, high(er), third, average, total 등과 같은 수량, 순서, 비교 등의 표현이 다수 등장했음. 아울러 political, democratic, republican, presidential 등의 정치 관련 표현도 예상대로 많이 등장함. """

draw_wordcloud(counter_adjs, f"{outcome_folder}/week6_wordcloud_adjs.png", 100)

"""### Popular Verbs Representing Behavioral Aspects"""

counter_verbs = get_counter(df, global_stopwords+local_stopwords, "VB")
counter_verbs.most_common(50)

"""동사로 한정지은 키워드 분석에서는 win(ning), lost 등과 같은 스포츠 경기 혹은 선거의 승리와 패배를 지칭하는 표현들로 여겨지는 키워드들이 눈에 띔."""

draw_wordcloud(counter_verbs, f"{outcome_folder}/week6_wordcloud_verbs.png", 100)

mask = df.body.str.contains("win", case=False)
df[mask][["datetime", "title", "body"]]

"""실제로 데이터를 확인해보아도 스포츠와 선거 모두의 승패와 관련된 것임을 알 수 있음.

## Time Series Analysis
"""

df.datetime.min(), df.datetime.max()

"""수집된 웹 페이지들이 게재된 기간은 2021년 9월 30일부터 2023년 1월 10일로서 약 16개월의 time range를 가짐. """

import numpy as np

count = pd.Series(data=1, index=df.datetime).resample(rule="w").count()
count

count.mean(), count.std()

"""주별 게재 건수의 평균은 약 20건, 표준편차는 약 5.6건으로서, 매주 평균 20건 정도가 게재되었음. """

count.plot(kind="line", title="Article Volume Change over Time", figsize=(15,7), grid=True)

"""### Peak Week Analysis"""

count.max(), count.idxmax()

"""가장 많이 게재된 peak week는 2022년 10월 24일 주간으로서, 미국 중간선거(11월 8일)를 앞두고 평균 20건을 크게 웃도는 총 33건의 글이 게재되었음.  """

df[(df.datetime >= "2022-10-24") & (df.datetime < "2022-10-31")][["datetime", "title", "body"]]

counter_peak = get_counter(df[(df.datetime >= "2022-10-24") & (df.datetime < "2022-10-31")], global_stopwords+local_stopwords, "NN")
counter_peak.most_common(30)

"""실제로 이 기간 동안 게재된 글들의 샘플과 키워드 랭킹을 살펴보면 상당 부분 미 중간선거와 관련된 내용임을 확인할 수 있음.

## Document Clustering
"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(use_idf=True, norm="l2", stop_words="english", max_df=0.7)
X = vectorizer.fit_transform(df.body)

X.shape

"""There are 1,347 documents and 35,977 words, or features.

### K-Means Clustering

### Step 1. Choose the number of clusters
"""

k = 5

"""### Step 2. Initialize a model object for k-means clustering"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=k, random_state=0)
kmeans

"""### Step 3. Fit the model using the input data"""

# Commented out IPython magic to ensure Python compatibility.
# %time kmeans.fit(X)

"""### Step 4. Examine the clustering outcome

The focus of this step should be on identifying the characteristics of each cluster.
"""

kmeans.labels_

df["cluster"] = kmeans.labels_

df[["title", "body", "cluster"]]

count2 = df.cluster.value_counts()
count2

"""전체 문서들에 대해 다양한 개수의 클러스터를 찾아본 결과 5개의 클러스터를 찾았을 때 비교적 균형 있는 클러스터들을 구할 수 있었음.  """

cluster_1st, cluster_2nd, cluster_3rd, cluster_4th, cluster_5th = df.cluster.value_counts().index

df[df.cluster == cluster_1st].sample(10, random_state=0)[["title", "body", "cluster"]]     # the largest cluster

counter_max = get_counter(df[df.cluster == cluster_1st], global_stopwords+local_stopwords)
counter_max.most_common(30)

"""가장 큰 클러스터는 스포츠에 대한 내용임을 쉽게 짐작할 수 있음. """

df[df.cluster == cluster_2nd].sample(10, random_state=0)[["title", "body", "cluster"]]     # the second largest cluster

counter_max = get_counter(df[df.cluster == cluster_2nd], global_stopwords+local_stopwords)
counter_max.most_common(30)

"""두번째로 큰 클러스터는 정치와 중간 선거에 대한 내용이 주를 이룸. """

df[df.cluster == cluster_5th].sample(10, random_state=0)[["title", "body", "cluster"]]     # the smallest cluster

counter_min = get_counter(df[df.cluster == cluster_5th], global_stopwords+local_stopwords)
counter_min.most_common(30)

"""가장 작은 클러스터는 미국에서 큰 논쟁을 일으켰던 낙태법 판결 대한 내용임.

## Topic Modeling

### Step 1. Choose the number of topics
"""

num_topics = 5

"""### Step 2. Initialize a model object for LDA topic modeling"""

from sklearn.decomposition import LatentDirichletAllocation as LDA

lda = LDA(n_components=num_topics, random_state=0)     # LDA uses randomness to get a probability distribution
lda

"""sklearn.decomposition.LatentDirichletAllocation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html

### Step 3. Fit the model using the input data
"""

# Commented out IPython magic to ensure Python compatibility.
# %time lda.fit(X)

"""### Step 4. Examine the output of topic modeling"""

def show_topics(model, feature_names, num_top_words):
    for topic_idx, topic_scores in enumerate(model.components_):
        print("*** Topic {}:".format(topic_idx))
        print(" + ".join(["{:.2f} * {}".format(topic_scores[i], feature_names[i]) for i in topic_scores.argsort()[::-1][:num_top_words]]))
        print()

show_topics(lda, vectorizer.get_feature_names_out(), 15)

"""### Topic Model Visualization"""

import pyLDAvis
import pyLDAvis.sklearn

pyLDAvis.enable_notebook()

"""pyLDAvis: https://github.com/bmabey/pyLDAvis"""

pyLDAvis.sklearn.prepare(lda, X, vectorizer)

"""다양한 개수의 토픽들을 시도해보아도 주로 2개의 큰 토픽과 서로 대부분 겹치는 작은 토픽들로 나누어지는 것을 확인할 수 있었음. 다양한 주제들이 다루어지는 사이트임을 감안했을 때 3개의 토픽 수는 적은 감이 있음. 이중 하나의 토픽은 스포츠, 또 다른 토픽은 정치(특히 대선)에 관한 내용임. """